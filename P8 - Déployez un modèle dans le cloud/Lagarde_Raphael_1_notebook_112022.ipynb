{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "17903fc9",
   "metadata": {},
   "source": [
    "# Projet 8 - Déployez un modèle dans le cloud"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb9502da",
   "metadata": {},
   "source": [
    "## 1/ Chargement des librairies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5701ddf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from io import BytesIO, StringIO\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import io\n",
    "import boto3\n",
    "\n",
    "from tensorflow.keras.applications.vgg16 import VGG16, preprocess_input\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "\n",
    "from pyspark.sql.functions import col, pandas_udf, PandasUDFType, udf, split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6491a4bb",
   "metadata": {},
   "source": [
    "## 2/ Création Session et Context Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "585951f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/home/ec2-user/.local/lib/python3.7/site-packages/pyspark/jars/spark-unsafe_2.12-3.2.2.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/11/23 09:20:34 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "          .master('local[*]') \\\n",
    "          .appName('sparkyfruitp8') \\\n",
    "          .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext\n",
    "sc.setLogLevel('ERROR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a81fcecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set('spark.sql.execution.arrow.pyspark.enabled', 'true')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9325b86",
   "metadata": {},
   "source": [
    "## 3/ Chargement des images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "515c1f31",
   "metadata": {},
   "source": [
    "### Lecture des images au format binaire dans notre bucket S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ebaa27d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.sql.dataframe.DataFrame'>\n",
      "root\n",
      " |-- path: string (nullable = true)\n",
      " |-- modificationTime: timestamp (nullable = true)\n",
      " |-- length: long (nullable = true)\n",
      " |-- content: binary (nullable = true)\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "s3path = 's3a://sparkyfruitp8/'\n",
    "\n",
    "imgS3 = spark.read.format('binaryFile')\\\n",
    ".option('pathGlobFilter', '*.jpg')\\\n",
    ".option('recursiveFileLookup', 'true').load(s3path).toDF('path', 'modificationTime', 'length', 'content')\n",
    "\n",
    "print(type(imgS3))\n",
    "print(imgS3.printSchema())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "72dcfc5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+------+--------------------+\n",
      "|                path|   modificationTime|length|             content|\n",
      "+--------------------+-------------------+------+--------------------+\n",
      "|s3a://sparkyfruit...|2022-11-16 10:36:06|  5969|[FF D8 FF E0 00 1...|\n",
      "|s3a://sparkyfruit...|2022-11-16 10:36:05|  5768|[FF D8 FF E0 00 1...|\n",
      "|s3a://sparkyfruit...|2022-11-16 10:36:06|  5654|[FF D8 FF E0 00 1...|\n",
      "|s3a://sparkyfruit...|2022-11-16 10:36:06|  5449|[FF D8 FF E0 00 1...|\n",
      "|s3a://sparkyfruit...|2022-11-16 10:36:04|  4984|[FF D8 FF E0 00 1...|\n",
      "+--------------------+-------------------+------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "imgS3.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3acf3dc",
   "metadata": {},
   "source": [
    "### Récupération du label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fa304a12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+-----+\n",
      "|                path|             content|label|\n",
      "+--------------------+--------------------+-----+\n",
      "|s3a://sparkyfruit...|[FF D8 FF E0 00 1...|Cocos|\n",
      "|s3a://sparkyfruit...|[FF D8 FF E0 00 1...|Cocos|\n",
      "|s3a://sparkyfruit...|[FF D8 FF E0 00 1...|Cocos|\n",
      "+--------------------+--------------------+-----+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "imgS3 = imgS3.withColumn('label', split(imgS3.path, '/')[3]).select(['path', 'content', 'label'])\n",
    "imgS3.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6709189f",
   "metadata": {},
   "source": [
    "## 4/ Chargement du CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aa048ae",
   "metadata": {},
   "source": [
    "### Suppression de la dernière couche de classification et récupération des poids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cfc61d8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-23 09:20:45.605376: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2022-11-23 09:20:45.605428: W tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:265] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-11-23 09:20:45.605454: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (ip-172-31-41-170.eu-west-3.compute.internal): /proc/driver/nvidia/version does not exist\n",
      "2022-11-23 09:20:45.605766: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "model = VGG16(include_top=False, pooling='max')\n",
    "\n",
    "bc_model_weights = sc.broadcast(model.get_weights())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d3b590e",
   "metadata": {},
   "source": [
    "## 5/ Création des fonctions pour l'extraction des features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36709640",
   "metadata": {},
   "source": [
    "### CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "69063839",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_fn():\n",
    "    \"\"\"\n",
    "    Returns a VGG16 model with top layer removed \n",
    "    and broadcasted pretrained weights.\n",
    "    \"\"\"\n",
    "    model_vgg = VGG16(weights=None, include_top=False, pooling='max')\n",
    "    model_vgg.set_weights(bc_model_weights.value)\n",
    "    return model_vgg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd13c4d2",
   "metadata": {},
   "source": [
    "### Preprocessing des images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8fbf9371",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def preprocess(content):\n",
    "    \"\"\"\n",
    "    Preprocesses raw image bytes for prediction.\n",
    "    \"\"\"\n",
    "    img = Image.open(io.BytesIO(content)).resize([224, 224])\n",
    "    arr = img_to_array(img)\n",
    "    return preprocess_input(arr)\n",
    "\n",
    "def featurize_series(model, content_series):\n",
    "    \"\"\"\n",
    "    Featurize a pd.Series of raw images using the input model.\n",
    "    :return: a pd.Series of image features\n",
    "    \"\"\"\n",
    "    input_ = np.stack(content_series.map(preprocess))\n",
    "    preds = model.predict(input_)\n",
    "    # For some layers, output features will be multi-dimensional \n",
    "    #tensors.\n",
    "    # We flatten the feature tensors to vectors for easier storage\n",
    "    #in Spark DataFrames.\n",
    "    output = [p.flatten() for p in preds]\n",
    "    return pd.Series(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfc6b6b5",
   "metadata": {},
   "source": [
    "### Fonction Pandas UDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0c52a9d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "@pandas_udf('array<float>', PandasUDFType.SCALAR_ITER)\n",
    "def featurize_udf(content_series_iter):\n",
    "    '''\n",
    "    This method is a Scalar Iterator pandas UDF wrapping \n",
    "    our featurization function.\n",
    "    The decorator specifies that this returns a \n",
    "    Spark DataFrame column of type ArrayType(FloatType).\n",
    "  \n",
    "    :param content_series_iter: This argument is an \n",
    "    iterator over batches of data, where each batch\n",
    "    is a pandas Series of image data.\n",
    "    '''\n",
    "    # With Scalar Iterator pandas UDFs, we can load \n",
    "    #the model once and then re-use it\n",
    "    # for multiple data batches.  This amortizes the \n",
    "    #overhead of loading big models.\n",
    "    model = model_fn()\n",
    "    for content_series in content_series_iter:\n",
    "        yield featurize_series(model, content_series)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f622ce5",
   "metadata": {},
   "source": [
    "## 6/ Extraction des features et enregistrement des résultats dans le bucket S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "590114dd",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "features_df = imgS3.repartition(1).select(col('path'), col('label'), featurize_udf('content').alias('features'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7e736ac",
   "metadata": {},
   "source": [
    "### Format parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3f9e454c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-23 09:20:49.900184: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-11-23 09:20:50.171725: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-11-23 09:20:50.171838: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2022-11-23 09:20:51.072843: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2022-11-23 09:20:51.072942: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2022-11-23 09:20:51.072955: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "2022-11-23 09:20:52.300610: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2022-11-23 09:20:52.300656: W tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:265] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-11-23 09:20:52.300683: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (ip-172-31-41-170.eu-west-3.compute.internal): /proc/driver/nvidia/version does not exist\n",
      "2022-11-23 09:20:52.300935: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-11-23 09:20:53.064921: W tensorflow/tsl/framework/cpu_allocator_impl.cc:82] Allocation of 308281344 exceeds 10% of free system memory.\n",
      "2022-11-23 09:20:53.297461: W tensorflow/tsl/framework/cpu_allocator_impl.cc:82] Allocation of 308281344 exceeds 10% of free system memory.\n",
      "1/1 [==============================] - 7s 7s/step\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "features_df.write.mode('overwrite').parquet('s3a://sparkyfruitp8-results/results_udf')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f9e01e0",
   "metadata": {},
   "source": [
    "### Format CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5e12dd07",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-23 09:21:03.479831: W tensorflow/tsl/framework/cpu_allocator_impl.cc:82] Allocation of 308281344 exceeds 10% of free system memory.\n",
      "2022-11-23 09:21:03.694937: W tensorflow/tsl/framework/cpu_allocator_impl.cc:82] Allocation of 308281344 exceeds 10% of free system memory.\n",
      "1/1 [==============================] - 7s 7s/step\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "results_df = features_df.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "718e779a",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_buffer = StringIO()\n",
    "results_df.to_csv(csv_buffer)\n",
    "# boto client\n",
    "client = boto3.client('s3')\n",
    "# put the object\n",
    "response = client.put_object(\n",
    "    Body=csv_buffer.getvalue(),\n",
    "    Bucket='sparkyfruitp8-results',\n",
    "    Key='results_udf.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
